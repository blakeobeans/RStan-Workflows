---
title: "RStan Workflows"
author: "Blake Shurtz"
link-citations: yes
linkcolor: blue
output:
  html_document:
    fig_caption: yes
    theme: spacelab #sandstone #spacelab #flatly
    highlight: pygments
    toc: TRUE
    toc_depth: 3
    number_sections: TRUE
    toc_float:
      smooth_scroll: FALSE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rethinking)
library(rstan)
```

```{r}
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
```

This is a kernel of regressions done in Rstan. Regression is the bread and butter of statistical analysis and Bayesian regression is superior due to the use of constraining priors and the generative ability of the model. Due to the Hamiltonian Monte Carlo algorithm, Stan is arguably the best way to do Bayesian regression. 

However, coding a regression model in Stan is not easy for a beginner and while there are plenty of advanced resources, there are few introductory resources. One exception is the excellent course "Statistical Rethinking" which uses and the *map2stan()* wrapper in the course's R package eponomously called *rethinking*. This kernel is an attempt to break away from the wrapper and futher understand Rstan, while still being able to verify results and compare models using *rethinking*'s *stancode()* function. 

#Simple Linear Regression With One Predictor

(Rethinking, Ch 4)

This module covers the basics of regression using RStan.

##Import Data

We are predicting height as a function of weight. We are centering weight around the sample mean. 

In order to separate the data from the analysis, it is best to export the data from R using the *stan_rdump()* function. We then read the data in using the *read_rdump* function. All of this is done in one code chunk.

```{r}
setwd("./2. linear regression")
data(Howell1)
d <- Howell1
d <- d[ d$age >= 18 , ]
xbar <- mean(d$weight)
N = length(d$weight)
height <- as.numeric(d$height)
weight <- as.numeric(d$weight)
stan_rdump(c("height", "N", "weight", "xbar"), file="linear_reg.data.R")
data <- read_rdump("linear_reg.data.R") #import data
```

##Model Design

###Prior Predictive Simulation

What are the reasonable prior assumptions on the beta coefficients? There is no wrong answer per se, but we want to *constrain* our priors to a sensible range and make them explicity in our analysis. We also don't want to look at our data when forming our prior.

The intercept will be the average height. Why? Because xi is centered on x-bar, when xi=x-bar, alpha = mu, and mu is the average height. We can assume based on prior knowledge that that the average height is around 178 cm We can also posit that height is normally distributed with a sd of 20 cm.

For the slope, we can assume that weight and height are positively correlated, but the strength of the association is unknown. Therefore, we can use the log-normal prior distribution to restrict the effects to positive values.

For a **prior predictive simulation**, we sample from the prior distribution. Simulates *N* regression lines for the effect of weight on height. 

```{r}
set.seed(2971)
N <- 100 # 100 lines
a <- rnorm( N , 178 , 20 ) #prior for alpha
b <- rlnorm( N , 0 , 1 ) #log normal prior for beta, to restrict effect of weight on height to be positive
#plot
plot(NULL, xlim=range(d$weight), ylim=c(-100,400), xlab="weight", ylab="height" )
abline(h=0, lty=2); abline(h=272, lty=1, lwd=0.5)
mtext("b ~ dlnorm(0,10)")
xbar <- mean(d$weight)
for (i in 1:N ) 
  curve(a[i] + b[i]*(x - xbar),
    from=min(d$weight), to=max(d$weight), add=TRUE, col=col.alpha("black",0.2))
```

##Stan Script

The Stan script is written outside of the environment and is read in using the code below.

There are three necessary blocks and one optional block. The three necessary blocks are data, parameters and model. They are always specified in that order.

The *data block* specifies the data. We can add a lower bound on the integers, such as with N. Note how the length of the variables are specified. 

The *parameters block* specifies the unobserved variables. We can put a lower bound on these as well.

The bulk of the work is in the *model block.* This is a *full model* meaning everything is specified. In order for the code to work, we have to specify any variables are we are creating in the model. The variable mu is the mean predicted reponse for the regression which is also the likelihood function. Recall that regressions n of the response), the slope and the intercept coefficients. We then use a for loop (not always required) to specify the likelihood function for each observation *i*. Finally, we model the distribution for the response, height,- a normal distribution.

The *generated quantities block* gives the distribution of the expectation for each weight/height combo (mu_i) as well as the predicted value for each weight/height combo (y_i).

```{r}
setwd("./2. linear regression")
writeLines(readLines("linear_regression.stan"))
```


We then execute the model. We can set the seed, the number of chains, the number of warmups and the number of iterations. Stan runs in C (it's faster than R) and R stores the output of the model in the Stan object called "fit". Let's sample from the posterior!

```{r}
setwd("./2. linear regression")
fit <- stan(file='linear_regression.stan', data=data, seed=194838, chains = 1, warmup=1000, iter=10000)
```

##Model Checking

We can get summary statistics for the posterior distributions. We can see that the average height is 154.6 and that for each additional kg of weight, height increases by .9 cm. The standard deviation of the expected height is 5.1 cm.

```{r}
print(fit, c("a", "b", "sigma")) #overview
```

When we convert the Stan object to a data frame, we get the individual samples from the posterior for each parameter and generated quantity for each iteration. For 10,000 iterations (subtracting 1000 warmup iterations) we get 9,000 samples from the posterior for each parameter and generated quantity. There are 3 parameters: alpha, beta and sigma, as well as 352 observations and thus 352 mu_i and 352 y_i for 707 columns. There is one more column for lp__ which is a measure of the fit of the model (log_posterior). More on that later.

```{r}
posterior <- as.data.frame(fit)
dim(posterior)
```
###Posterior Distribution of the Slope Coefficient 

We can plot the distribution of any of the parameters by plotting the samples. Below is the distribution of the slope coefficient. We can also show the mean and the shaded interval that contains 95% of the density.

```{r}
b.mean <- mean(posterior$b)
b.HPDI <- HPDI(posterior$b, prob = .95)
plot(density(posterior$b), main = "Posterior for Slope")
shade(density(posterior$b), c(b.HPDI[[1]], b.HPDI[[2]]))
abline(v=b.mean)
```

###Visualizing the Posterior Regression Line

The code below plots the data, a regression line that uses the mean of the distributions for alpha and beta, and 20 individual regression lines using 20 samples from the posterior.

```{r}
plot(d$height ~ d$weight, col=rangi2)
curve(mean(posterior$a) + mean(posterior$b) * (x-xbar), add = TRUE, col = 'red') #x not specified... weird...
#add more lines
samples <- posterior[1:20,]
for ( i in 1:20 )
  curve( samples$a[i] + samples$b[i]*(x-xbar), col=col.alpha("black",0.3) , add=TRUE)
```


###Comparing the Prior and Posterior Regression Lines

Below we plot 20 priors (using the same code as before), as well as 20 posterior samples and the mean posterior regression line.

```{r}
###code from before (the prior)
set.seed(2971)
N <- 20 # 100 lines
a <- rnorm( N , 178 , 20 ) #prior for alpha
b <- rlnorm( N , 0 , 1 ) #log normal prior for beta, to restrict effect of weight on height to be positive
#plot
plot(NULL, xlim=range(d$weight), ylim=c(-100,400), xlab="weight", ylab="height" )
abline(h=0, lty=2); abline(h=272, lty=1, lwd=0.5)
mtext("b ~ dlnorm(0,10)")
xbar <- mean(d$weight)
for (i in 1:N ) 
  curve(a[i] + b[i]*(x - xbar),
    from=min(d$weight), to=max(d$weight), add=TRUE, col=col.alpha("black",0.2))
###code from after (the posterior)
#samples of beta from the posterior distribution
samples <- posterior[1:20,]
for ( i in 1:20 )
  curve( samples$a[i] + samples$b[i]*(x-xbar), col=col.alpha("blue",0.3) , add=TRUE)
#posterior mean for the slope coefficient
curve(mean(posterior$a) + mean(posterior$b)*(x-xbar), col = 'red', add = TRUE) 
```

## Two key functions- Link and Sim

###Confidence intervals and the Link Function

Assuming you're familiar with the CI for a regression, we use a slightly different method when running Stan. Note that white the rethinking package offers the link function, but it only works on *map2stan* objects, not *stanfit* objects. Instead, we'll write our own function.

First, we define a function, in this case, the regression function.

Then, we want to introduce the data we want to predict from. This can be the existing data for which we know the response or we could have new data where we do not know the response. Or, if we want to build a confidence interval, we use the domain of possible weights in a sequence. 

Finally, we use *sapply()* to run the weight sequence through the function.

That's it! We have our predictions. Lastly, we calculate the mean and prediction intervals (density intervals can be used as well).

```{r}
mu.link <- function(weight) posterior$a + posterior$b*( weight - xbar )
weight.seq <- seq( from=25 , to=70 , by=1 )
mu <- sapply( weight.seq , mu.link )
dim(mu) #same rows as iterations; same column as sequence
#calculate mean and HDPI
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI , prob=0.89 )
#plot
plot( height ~ weight , data=d , col=col.alpha(rangi2,0.5) ) # plot raw data 
lines( weight.seq , mu.mean ) # plot the posterior regression line
shade( mu.PI , weight.seq ) # plot a shaded region for 89% HPDI
```

Alternatively, you cancreate a matrix object with a column for each X value and as many rows from our posterior as we desire. For each row, run each x value through the regression equation and store it in the correct cell of the matrix.

```{r, eval = FALSE}
weight.seq <- seq( from=25 , to=70 , by=1 )
matrix_rows = nrow(posterior)
matrix_cols = length(weight.seq)
obj <- matrix(0, nrow=matrix_rows, ncol=matrix_cols)
for (i in 1:matrix_rows)
  for (j in 1:matrix_cols)
    obj[i,j] <- (samples$a[i] + samples$b[i]*(weight.seq[j]-xbar))
```

#### Univariate Conditional Distribution for the Expectation of Y

You can also use the link function to plot the univariate distribution for a given value of x.

```{r}
mu.link <- function(weight) posterior$a + posterior$b*( weight - xbar )
weight_80 <- 80
mu_80 <- sapply( weight_80 , mu.link )
dim(mu_80)
#calculate mean and HDPI
mu.80.mean <- apply( mu_80 , 2 , mean )
mu.80.PI <- apply( mu_80 , 2 , PI , prob=0.89 )
#plot
plot(density(mu_80), main = "Mu at 80")
shade(density(mu_80), c(mu.80.PI[[1]], mu.80.PI[[2]]))
abline(v=mu.80.mean)
```


### Prediction Intervals and the Sim function

Also known as posterior predictive simulation, which is estimating the range of possible responses to new data or seeing how well your existing data fits. 

The key idea behind a prediction interval is that we estimate the range of possible values, incorporating not only the uncertainty in the expected value but the uncertainty in the distribution of the response.

Rather than sampling from the likelihood for the expected value (as with link), for each weight in the sequence, we sample from the distribution for the heights. We simulate draws from this distribution using the rnorm function where the mean is the likelihood function and the sd is the posterior estimated sd.

```{r}
weight.seq <- 25:70
sim.height <- sapply( weight.seq , function(weight)
  rnorm(
    n=nrow(posterior) ,
    mean=posterior$a + posterior$b*( weight - xbar ) ,
    sd=posterior$sigma ) )
#summary stats
dim(sim.height)
height.PI <- apply( sim.height , 2 , PI , prob=0.89 ) #you can also use quartile
height.mean <- apply( sim.height , 2 , mean , prob=0.89 )
#
plot( height ~ weight , d, col=col.alpha(rangi2,0.5), main="Prediction Interval" ) #plot data
lines( weight.seq , height.mean ) # regression line
shade( height.PI , weight.seq ) # prediction interval
shade( mu.PI , weight.seq ) # confidence interval
```

####Univariate Conditional Distribution of for the Predicted Value of Y

```{r}
weight.80 <- 80
y_80_sim <- sapply( weight.80 , function(weight)
  rnorm(
    n=nrow(posterior) ,
    mean=posterior$a + posterior$b*( weight - xbar ) ,
    sd=posterior$sigma ) )
#calculate mean and HDPI
y.80.mean <- apply( y_80_sim , 2 , mean )
y.80.PI <- apply( y_80_sim , 2 , PI , prob=0.89 )
#plot
plot(density(y_80_sim), main = "Y at 80")
shade(density(y_80_sim), c(y.80.PI[[1]], y.80.PI[[2]]))
abline(v=y.80.mean)
```


#Multivariate Regression with Standardized, Continuous Predictors 

(Rethinking, Chapter 5)

Correlation between predictors is the norm. Multiple regression is about distinguising correlations and assoication from evidence for causation. This means building DAGs, thinking about confounders, etc.

##Import data

In this example, we're looking at state-level data as to whether the divorce rate is predicted by the average age of marriage and the proportion of married individuals in the state.

```{r}
setwd("./3. Multivariate Continuous")
data(WaffleDivorce)
d <- WaffleDivorce
age <- as.numeric(d$MedianAgeMarriage)
divorce <- as.numeric(d$Divorce)
N <- length(d$Divorce)
marriage_proportion <- as.numeric(d$Marriage) #proportion
stan_rdump(c("divorce", "age", "N", "marraige_proportion"), file="linear_reg.data.R")
data <- read_rdump("linear_reg.data.R")
```


##Stan Script

In this case, we're going to standardize our data with the additional **transformed data** block. This can be done in R, but it's better to do in Stan.

Because our variables are standardized, the intercept should be close to 0 and the slopes have a "sd" interpretation- a slope of 1 suggests a 1 sd increase in X leads to a 1 sd increase in y. The priors reflect this.

```{r}
setwd("./3. Multivariate Continuous")
writeLines(readLines("stan_model.stan"))
```

```{r}
setwd("./3. Multivariate Continuous")
fit <- stan(file='stan_model.stan', data=data, seed=194838, chains = 1, warmup=1000, iter=10000)
```

## Model Checking

```{r}
print(fit, c("a", "bM", "bA", "sigma"))
```

### Plot Coefficients

```{r}
plot(fit, pars = c("a", "bA", "bM", "sigma"), ci_level = .8, show_density = TRUE) + 
  ggtitle("Posterior Distribution of Coefficients")
```

```{r}
posterior <- as.data.frame(fit)
```

With multivariate regression, there are three additional types of plots: predictor residual plots, counterfactual plots and posterior predictive plots.

### Predictor Residual Plot

Unlike a standard residual plot, a *predictor residual plot* plots the residuals for a single predictor against the response. It shows the average prediction error after statistically adjusting for the predictor of interest. To compute the predictor residuals for one predictor, just re-run the model excluding the predictor of interest. In the example below, we want the residuals after factoring out marriage.

You can use this technique more generally for one or more predictors.

```{r}
setwd("./3. Multivariate Continuous")
writeLines(readLines("stan_model2.stan"))
fit2 <- stan(file='stan_model2.stan', data=data, seed=194838, chains = 1, warmup=1000, iter=10000)
```

You can think of this plot as displaying the linear relationship between divorce and marriage rates, having
statistically “controlled” for median age of marriage. (134) So States to the right of the vertical line have higher marriage rates than expected. The regression line demonstrates little relationship between divorce and marriage rates.


```{r}
posterior2 <- as.data.frame(fit2)
mu.link <- function(x) posterior2$a + posterior2$bA*(x)
mu <- sapply(as.numeric(scale(d$MedianAgeMarriage)), mu.link)
mu_mean <- apply(mu, 2, mean)
mu_resid <- as.numeric(scale(d$Marriage)) - mu_mean
mu.PI <- apply( mu , 2 , HPDI , prob=0.89 )
#plot
plot(as.numeric(scale(d$Divorce)) ~ mu_resid, main = 'Predictor Residual Plot', xlab="Marriage rate residuals", ylab = "Divorce Rate (std)")
abline(lm(as.numeric(scale(d$Divorce)) ~ mu_resid), col = 'red') #x not specified... weird...
shade( mu.PI , as.numeric(scale(d$MedianAgeMarriage)) ) # plot a shaded region for 89% HPDI
abline(v = 0)
```


###Counterfactual Plots

A second sort of inferential plot displays the implied predictions of the model. I call these plots counterfactual, because they can be produced for any values of the predictor variables you like, even unobserved or impossible combinations like very high median age of marriage and very high marriage rate.

The simplest use of a counterfactual plot is to see how the predictions change as you
change only one predictor at a time. This means holding the values of all predictors constant,
except for a single predictor of interest. Such predictions will not necessarily look like
your raw data—they are counterfactual after all—but they will help you understand the implications
of the model.

```{r}
mu.link <- function(A, M) posterior$a + posterior$bA*(A) + posterior$bM * M
# prepare new counterfactual data
M_seq <- seq( from=-2 , to=3 , length.out=30 )
pred_data <- data.frame( M = M_seq , A = 0 )
#compute counterfactual mean divorce (mu)
mu <- mapply(mu.link, M = pred_data$M, A = pred_data$A) #note use of mapply
mu_mean <- apply( mu , 2 , median )
mu_PI <- apply( mu , 2 , PI )
#compute counterfactual predicted divorce (y)
sim.response <- mapply(function(A, M)
  rnorm(
    n=nrow(posterior) ,
    mean=posterior$a + posterior$bA*(A) + posterior$bM * M,
    sd=posterior$sigma ), M = pred_data$M, A = pred_data$A   )
#summary stats
dim(sim.response)
D_PI <- apply( sim.response , 2 , PI , prob=0.89 ) #you can also use quartile
# display predictions, hiding raw data with type="n"
plot( as.numeric(scale(d$Divorce)) ~ as.numeric(scale(d$Marriage)) , data=d , type="n",
      xlab = "Marriage rate (standardized)", ylab = "Divorce rate (standardized)")
mtext( "Median age marriage (std) = 0" )
lines( M_seq , mu_mean )
shade( mu_PI , M_seq )
shade( D_PI , M_seq )
```



###Posterior Prediction Plot

In addition to understanding the posterior distribution of the parameters, it’s important to check the model’s implied predictions against the observed data. 

Did the model correctly approximate the posterior distribution? Golems do make
mistakes, as do golem engineers. Errors can be more easily diagnosed by comparing
implied predictions to the raw data. Some caution is required, because not all
models try to exactly match the sample.

How does the model fail? All models are useful fictions, so they always fail in some
way. Sometimes, the model fits correctly but is still so poor for our purposes that it
must be discarded. More often, a model predicts well in some respects, but not in
others. By inspecting the individual cases where the model makes poor predictions,
you might get an idea of how to improve the model. The difficulty is that this mode
is essentially creative and relies upon the analysts domain expertise. It also creates
risks of chasing noise in the sample, a topic we’ll focus on in later chapters.

Note this is for the dependent variable.

```{r}
mu.link <- function(A, M) posterior$a + posterior$bA*(A) + posterior$bM * M
#using original data
mu <- mapply(mu.link, M = as.numeric(scale(d$Marriage)), A= as.numeric(scale(d$MedianAgeMarriage))) #note use of mapply
mu_mean <- apply( mu , 2 , mean )
mu_PI <- apply( mu , 2 , PI )
# simulate observations
# again no new data, so uses original data
sim.response <- mapply(function(A, M)
  rnorm(
    n=nrow(posterior) ,
    mean=posterior$a + posterior$bA*(A) + posterior$bM * M,
    sd=posterior$sigma ), M = as.numeric(scale(d$Marriage)), A = as.numeric(scale(d$MedianAgeMarriage))   )
#summary stats
D_PI <- apply( sim.response , 2 , PI , prob=0.89 ) #you can also use quartile
#plot
plot( mu_mean ~ as.numeric(scale(d$Divorce)) , col=rangi2 , ylim=range(mu_PI) ,
xlab="Observed divorce" , ylab="Predicted divorce" )
abline( a=0 , b=1 , lty=2 )
for ( i in 1:nrow(d) ) lines( rep(scale(d$Divorce)[i],2) , mu_PI[,i] , col=rangi2 )
```



#Multivariate Regression with a Single Categorical Variable

Data is on different kinds of monkeys ("clade") and the kilocalories of milk. 


##Data

```{r}
setwd("./5. Single Categorical")
data(milk)
d <- milk
unique(d$clade)
clade_id <- as.integer(d$clade) #convert categorical variable to integer
N <- length(clade_id)
N_clade_id <- length(unique(clade_id))
K <- scale(d$kcal.per.g)
K <- as.numeric(K)
stan_rdump(c("clade_id", "N_clade_id", "N", "K"), file="linear_reg.data.R")
data <- read_rdump("linear_reg.data.R")
```

##Stan Model

Note that all of the categories have the same prior. The regression is just mu = the categorical variable.

```{r}
setwd("./5. Single Categorical")
writeLines(readLines("categorical.stan"))
```

```{r}
setwd("./5. Single Categorical")
fit <- stan(file='categorical.stan', data=data, seed=194838, 
            chains = 1, warmup=1000, iter=10000)
```

##Analysis

```{r}
posterior <- as.data.frame(fit)
print(fit, c("sigma", "a"))
plot(fit, pars = c("a", "sigma"), ci_level = .8, outer_level = 0.95, show_density = TRUE) + 
  ggtitle("Posterior Distribution of Coefficients")
```

#Instrumental Variables

From chapter 8.

##Data
```{r}
setwd("./7. Instrumental Variables")
set.seed(73)
N <- 200
U <- rnorm( N ) #U not actually included as data
Q <- rbern( N , prob=0.25 ) #out of rethinking package...
E <- rnorm( N , U - Q ) #E is a fn of U
W <- rnorm( N , U + 0*E ) #W is a fn of U
id <- 1:N #dummy index for person
stan_rdump(c("N", "Q", "E", "W", "id"), file="iv_reg.data.R")
data <- read_rdump("iv_reg.data.R")
```

##Model
```{r}
setwd("./7. Instrumental Variables")
writeLines(readLines("iv_code.stan"))
```

Fit model

```{r}
setwd("./7. Instrumental Variables")
fit <- stan(file='iv_code.stan', data=data, chains = 1, warmup=1000, iter=10000)
```

##Analysis

```{r}
print(fit, c("aW", "bEW", "sigmaW", "aE", "bQE", "sigmaE"))
```

#Multi-level Model



##Data

```{r}
setwd("./13. Multi-Level Models")
data(reedfrogs)
d <- reedfrogs
surv <- d$surv
density <- d$density
tank <- 1:nrow(d)
N = nrow(d)
N_tank = nrow(d)
stan_rdump(c("N", "N_tank", "surv", "density", "tank"), file="linear_reg.data.R")
data <- read_rdump("linear_reg.data.R")
```

##Stan Model

```{r}
setwd("./13. Multi-Level Models")
writeLines(readLines("m12_1.stan"))
```

It is worth noting the difference between a standard regression and multi-level regression at the level of the code. 

In the standard regression, all "varying intercept" coefficients for the categorical variable have the same normal prior. Thus, no information is shared among the coefficients.  

In the multi-level model, the prior has a hyperprior (an "adaptive regularizing prior"), a_tank ~ N(alpha,sigma), with two hyperparameters, a and sigma. The hyperprior models the population of all groups- it makes explicit that all of the intercepts come from the same population. In order to determine the value of the hyperparameters, information from all of the varying intercepts must be pooled. This is done simultaneously with determining the varying intercepts, brought to you by MCMC.

```{r}
setwd("./13. Multi-Level Models")
writeLines(readLines("stan_script.stan"))
```

```{r}
setwd("./13. Multi-Level Models")
fit <- stan(file='stan_script.stan', data=data, chains = 1, warmup=1000, iter=10000)
```

##Analysis

```{r}
print(fit, c("a", "sigma"))
```

