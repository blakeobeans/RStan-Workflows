---
title: "RStan Workflows"
author: "Blake Shurtz"
link-citations: yes
linkcolor: blue
output:
  html_document:
    fig_caption: yes
    theme: spacelab #sandstone #spacelab #flatly
    highlight: pygments
    toc: TRUE
    toc_depth: 3
    number_sections: TRUE
    toc_float:
      smooth_scroll: FALSE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(rethinking)
library(rstan)
```

```{r}
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
```

This computational essay is a collection of best practice analysis using Stan and RStan. It assumes that the reader understands the basics of regression and Bayesian statistics. In order to keep things brief, some later examples do not include all of the recommended analysis done in earlier examples.

#Simple Linear Regression With One Predictor

(Rethinking, Ch 4)

##Data

This is a cross-sectional data set with two variables: height and weight. The measurements are for adults only. 

We are predicting height as a function of weight. We are centering weight around the sample mean, hence the use of xbar. 

In order to separate the data from the analysis, it is best to export the data from R using the *stan_rdump()* function in the *RStan* package. Note that each variable is included separately. After exporting the data, we then read it in prior to analysis using the *read_rdump* function. 

```{r}
setwd("./2. linear regression")
data(Howell1)
d <- Howell1
d <- d[ d$age >= 18 , ]
xbar <- mean(d$weight)
N = length(d$weight)
height <- as.numeric(d$height)
weight <- as.numeric(d$weight)
stan_rdump(c("height", "N", "weight", "xbar"), file="linear_reg.data.R")
data <- read_rdump("linear_reg.data.R") #import data
```

##Prior Predictive Simulation

**Prior predictive simulation** (PPS) falls under the heading of *model design*. For PPS, the question is- what are the reasonable prior assumptions on the  regression coefficients? 

A few general tips: there is no wrong answer except to skip this step. Note that we're discussing priors on the coefficients, not the variables themselves. Generally, we want to *constrain* our priors to a sensible range- we wouldn't expect weight to be negatively correlated with height, for example. Think of the relation of the prior to any transformations for the data. We don't want to look at our data when forming the prior. We want to explicity mention our prior in the analysis. 

The **intercept** represents the average height because xi is centered on x-bar. Therefore, we can think of the prior as the distribution of heights- ie. the variable itself. (In the regression equation, when xi=x-bar, beta drops out and alpha = mu, the average height.) Based on prior knowledge, we can posit a normal prior with a mean of 178 cm and an sd of 20. We can plot this distrubition.

```{r}
N <- 100 # 100 lines
a <- rnorm( N , 178 , 20 ) #prior for alpha
plot(density(a), main = "Prior for Alpha", xlab = "Height in cm")
```

For the **slope coefficient**, we can assume that weight and height are positively correlated, but the strength of the association is unknown. Therefore, we can use the log-normal prior distribution to restrict the coefficent to positive values.

```{r}
set.seed(2971)
N <- 100 # 100 lines
a <- rnorm( N , 178 , 20 ) #prior for alpha
b <- rlnorm( N , 0 , 1 ) #log normal prior for beta, to restrict effect of weight on height to be positive
#plot
plot(NULL, xlim=range(d$weight), ylim=c(-100,400), xlab="weight", ylab="height" )
abline(h=0, lty=2); abline(h=272, lty=1, lwd=0.5)
mtext("b ~ dlnorm(0,10)")
xbar <- mean(d$weight)
for (i in 1:N ) 
  curve(a[i] + b[i]*(x - xbar),
    from=min(d$weight), to=max(d$weight), add=TRUE, col=col.alpha("black",0.2))
```

##Stan Model

The Stan script is written outside of the environment and is read in using *readLines* in base R.

In a Stan script, there are three necessary "blocks" and one optional block. The three necessary blocks are data, parameters and model. They are always specified in that order.

The *data block* specifies the data. It should correspond to the data set we created in the earlier section. It includes vectors- the variables- as well as individual scalars. We can add a lower bound on the scalars, such as with N. Note how the length of the variables are specified. 

The *parameters block* specifies the unobserved variables that we'd like to estimate, in this case, alpha, beta and sigma. A lower bound is placed on sigma because there is no negative standard deviation.

The bulk of the work is in the *model block.* This is a *full model* meaning everything is specified. The model is laid out in a few sections.

First, we have to specify any variables are created within the model. In this case, the variable mu is the mean expected reponse for the regression.
Second, we lay out our priors.
Third, we specify the regression function. In this case, it's done with a for loop. While these loops are optional, personally it helps clarify the math.
Finally, we lay out the likelihood function. We should read this as the height *for each observation* is a function of the mean *for each observation* with shared variance sigma.

The optional *generated quantities block* provides other quantities in the output, ie. other posterior distributions. There are generally two outputs we're interested in: the expectations and the predictions. For each observation, we want the distribution of the expectation for each weight/height combo (mu_i). Also, for each observation, we want the distribution of the predicted value for each weight/height combo (y_i). The latter will have a wider variance. The *normal_rng* function is used to sample from the likelihood.

```{r}
setwd("./2. linear regression")
writeLines(readLines("linear_regression.stan"))
```

Time to execute the model. We can set the seed, the number of chains, the number of warmups and the number of iterations. Note that the number of posterior samples is equal to the number of iterations minus the number of warmups, in this case, 9000. Stan runs in C (C is fast) and R stores the output of the model in the Stan object called "fit". Let's sample from the posterior!

```{r}
setwd("./2. linear regression")
fit <- stan(file='linear_regression.stan', data=data, seed=194838, chains = 1, warmup=1000, iter=10000)
```

##Model Checking

Stanfit objects are lists that contain lots of information, but primarily they are just samples from the posterior.

### Parameters Summary Table

We can get summary statistics for variables of interest using *print()* and specifying the variables of interest. We can see that the average height is 154.6 and that for each additional kg of weight, height increases by .9 cm. The standard deviation of the expected height is 5.1 cm.

```{r}
print(fit, c("a", "b", "sigma"),  probs = c(0.10, 0.5, 0.9)) #overview
```

### Parameter Posterior Distributions

We can plot the parameters using *plot()*. The standard errors are so low that the distributions are laughably narrow.

```{r}
plot(fit, pars = c("a", "b", "sigma"), ci_level = .95, show_density = TRUE) + 
  ggtitle("Posterior Distribution of Parameters")
```

We can plot the distribution of any of the parameters by plotting the samples. Converting a fitstan object to a data frame allows us to access the individual samples. We get the all of the individual samples from the posterior for each parameter and generated quantity. There are 3 parameters: alpha, beta and sigma, as well as 352 observations and thus 352 mu_i and 352 y_i for 707 columns. There is one more column for lp__ which is a measure of the fit of the model (log_posterior). More on that later.

```{r}
posterior <- as.data.frame(fit)
dim(posterior)
```

Below is the distribution of the slope coefficient. Note that it should match the plot above. We can also show the mean and the shaded interval that contains 95% of the density.

```{r}
b.mean <- mean(posterior$b)
b.HPDI <- HPDI(posterior$b, prob = .95)
plot(density(posterior$b), main = "Posterior for Slope")
shade(density(posterior$b), c(b.HPDI[[1]], b.HPDI[[2]]))
abline(v=b.mean)
```

### Prior and Posterior Coefficients

We can compare the prior and posterior for the slope coefficient. Below we plot 20 priors (using the same code as before), as well as 20 posterior samples and the mean posterior regression line.

```{r}
###code from before (the prior)
set.seed(2971)
N <- 20 # 100 lines
a <- rnorm( N , 178 , 20 ) #prior for alpha
b <- rlnorm( N , 0 , 1 ) #log normal prior for beta, to restrict effect of weight on height to be positive
#plot
plot(NULL, xlim=range(d$weight), ylim=c(-100,400), xlab="weight", ylab="height", main= "Prior and Posterior Regression Line" )
abline(h=0, lty=2); abline(h=272, lty=1, lwd=0.5)
xbar <- mean(d$weight)
for (i in 1:N ) 
  curve(a[i] + b[i]*(x - xbar),
    from=min(d$weight), to=max(d$weight), add=TRUE, col=col.alpha("black",0.2))
###code from after (the posterior)
#samples of beta from the posterior distribution
samples <- posterior[1:20,]
for ( i in 1:20 )
  curve( samples$a[i] + samples$b[i]*(x-xbar), col=col.alpha("blue",0.3) , add=TRUE)
#posterior mean for the slope coefficient
curve(mean(posterior$a) + mean(posterior$b)*(x-xbar), col = 'red', add = TRUE) 
```

### Posterior Regression Lines

The code below plots the data, a regression line that uses the mean of the distributions for alpha and beta, and 20 individual regression lines using 20 samples from the posterior.

```{r}
plot(d$height ~ d$weight, col=rangi2)
curve(mean(posterior$a) + mean(posterior$b) * (x-xbar), add = TRUE, col = 'red') #x not specified... weird...
#add more lines
samples <- posterior[1:20,]
for ( i in 1:20 )
  curve( samples$a[i] + samples$b[i]*(x-xbar), col=col.alpha("black",0.3) , add=TRUE)
```


### Confidence intervals and the Link Function

Assuming you're familiar with the CI for a regression, we use a slightly different method when running Stan. Note that while the *rethinking* package offers the link function, but it only works on *map2stan* objects, not *stanfit* objects. Instead, we'll write our own function.

First, we define a function, in this case, the regression function.

Then, we want to introduce the data we want to predict from. This can be the existing data for which we know the response, or we could have new data where we do not know the response. Or, if we want to build a confidence interval, we use the domain of possible weights in a sequence. 

Finally, we use *sapply()* to run the weight sequence through the function.

That's it! We have our predictions. Lastly, we calculate the mean and prediction intervals (density intervals can be used as well).

```{r}
mu.link <- function(weight) posterior$a + posterior$b*( weight - xbar )
weight.seq <- seq( from=25 , to=70 , by=1 )
mu <- sapply( weight.seq , mu.link )
#dim(mu) #number of rows = number of samples; # of columns = number of observations in sequence
#calculate mean and HDPI
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI , prob=0.89 )
#plot
plot( height ~ weight , data=d , col=col.alpha(rangi2,0.5) ) # plot raw data 
lines( weight.seq , mu.mean ) # plot the posterior regression line
shade( mu.PI , weight.seq ) # plot a shaded region for 89% HPDI
```

#### Univariate Conditional Distribution for the Expectation of Y

You can also use the link function to plot the univariate distribution for a given value of x.

```{r}
mu.link <- function(weight) posterior$a + posterior$b*( weight - xbar )
weight_80 <- 80
mu_80 <- sapply( weight_80 , mu.link )
#calculate mean and HDPI
mu.80.mean <- apply( mu_80 , 2 , mean )
mu.80.PI <- apply( mu_80 , 2 , PI , prob=0.89 )
#plot
plot(density(mu_80), main = "Mu at 80")
shade(density(mu_80), c(mu.80.PI[[1]], mu.80.PI[[2]]))
abline(v=mu.80.mean)
```


### Prediction Intervals and the Sim Function

Also known as *posterior predictive simulation*, which is estimating the range of possible responses to new data or seeing how well your existing data fits. 

The key idea behind a prediction interval is that we estimate the range of possible values, incorporating not only the uncertainty in the expected value but the uncertainty in the likelihood function.

Rather than sampling from the regression (as with link), we sample from the likelihood for the heights. We simulate draws from this distribution using the *rnorm()* function where the mean is the likelihood function and the sd is the posterior estimated sd.

```{r}
weight.seq <- 25:70
sim.height <- sapply( weight.seq , function(weight) #see function below
    rnorm(
      n=nrow(posterior) ,
      mean=posterior$a + posterior$b*( weight - xbar ) ,
      sd=posterior$sigma ) 
    )
#summary stats
height.PI <- apply( sim.height , 2 , PI , prob=0.89 ) #you can also use quartile
height.mean <- apply( sim.height , 2 , mean , prob=0.89 )
#
plot( height ~ weight , d, col=col.alpha(rangi2,0.5), main="Prediction Interval" ) #plot data
lines( weight.seq , height.mean ) # regression line
shade( height.PI , weight.seq ) # prediction interval
shade( mu.PI , weight.seq ) # confidence interval
```

#### Univariate Conditional Distribution of for the Predicted Value of Y


```{r}
weight.80 <- 80
y_80_sim <- sapply( weight.80 , function(weight)
  rnorm(
    n=nrow(posterior) ,
    mean=posterior$a + posterior$b*( weight - xbar ) ,
    sd=posterior$sigma ) )
#calculate mean and HDPI
y.80.mean <- apply( y_80_sim , 2 , mean )
y.80.PI <- apply( y_80_sim , 2 , PI , prob=0.89 )
#plot
plot(density(y_80_sim), main = "Y at 80")
shade(density(y_80_sim), c(y.80.PI[[1]], y.80.PI[[2]]))
abline(v=y.80.mean)
```


#Multivariate Regression with Standardized, Continuous Predictors 

(Rethinking, Chapter 5)

Correlation between predictors is the norm. Multiple regression is about distinguising correlations and assoication from evidence for causation. This means building DAGs, thinking about confounders, etc.

##Data

In this example, we're looking at cross-sectional state-level data as to whether the divorce rate is predicted by the average age of marriage and the proportion of married individuals in the state. As there are 50 states, there are 50 observations.

```{r}
setwd("./3. Multivariate Continuous")
data(WaffleDivorce)
d <- WaffleDivorce
median_age_marriage <- as.numeric(d$MedianAgeMarriage)
divorce <- as.numeric(d$Divorce)
N <- length(d$Divorce)
state_marriage_proportion <- as.numeric(d$Marriage) #proportion
stan_rdump(c("divorce", "median_age_marriage", "N", "state_marriage_proportion"), file="linear_reg.data.R")
data <- read_rdump("linear_reg.data.R")
```


##Stan Model

In this case, we're going to standardize our data with the additional **transformed data** block. This can be done in R, but it's arguably better to do in Stan.

Because our variables are standardized, the intercept should be close to 0 and the slopes have a "sd" interpretation- a slope of 1 suggests a 1 sd increase in X leads to a 1 sd increase in y. The priors reflect this.

```{r}
setwd("./3. Multivariate Continuous")
writeLines(readLines("stan_model.stan"))
```

```{r}
setwd("./3. Multivariate Continuous")
fit <- stan(file='stan_model.stan', data=data, seed=194838, chains = 1, warmup=1000, iter=10000)
```

## Model Checking

With multivariate regression, there are three additional types of plots: predictor residual plots, counterfactual plots and posterior predictive plots. These plots focus on the ability to "statistically control" variables within a model.

### Predictor Residual Plot

A **predictor residual plot** (PRP) plots the residuals for a single predictor against the response. A PRP shows the average prediction error after statistically adjusting for the predictor(s) of interest. You can think of it as a residual plot where you select which variables are endogenous to the model. It allows you to measure the "ceteris paribus" association between a predictor(s) and the response. 

To compute the predictor residuals, re-run the model excluding the predictor(s) of interest. In the example below, we want to see the "ceteris paribus" response of the divorce rate to the state-level marriage proportion, adjusting for the median age of marraige.

```{r}
setwd("./3. Multivariate Continuous")
writeLines(readLines("stan_model2.stan"))
fit2 <- stan(file='stan_model2.stan', data=data, seed=194838, chains = 1, warmup=1000, iter=10000)
```

We use a link function to plug in our data and calculate the mean effect of the median age of marriage in each state. Then - and this is key - we subtract this mean effect from the state-level marriage proportion to create the predictor residuals.

We then plot the divorce rate against the predictor residuals, along with a line of best fit and a regression (ie. confidence) interval. You can think of the plot as a 2 x 2 matrix, for example, states to the right of the vertical line and above the regression line have higher marriage rates than expected and higher divorce rates. Overall, we do not see a strong association between state-level marriage proportions and divorce rates. (Note: there is a misalignment in the regression line and the regression interval- not sure why.)


```{r}
posterior2 <- as.data.frame(fit2)
#create link function
mu.link <- function(x) posterior2$a + posterior2$bA*(x)
mu <- sapply(as.numeric(scale(d$MedianAgeMarriage)), mu.link)
  #summary stasitics for median 
  mu_mean <- apply(mu, 2, mean)
  mu_resid <- as.numeric(scale(d$Marriage)) - mu_mean
  mu.PI <- apply( mu , 2 , HPDI , prob=0.9 )
#plot
plot(as.numeric(scale(d$Divorce)) ~ mu_resid, main = 'Predictor Residual Plot', xlab="Marriage rate residuals", ylab = "Divorce Rate (std)")
abline(lm(as.numeric(scale(d$Divorce)) ~ mu_resid), col = 'red') #x not specified... weird...
shade( mu.PI , as.numeric(scale(d$MedianAgeMarriage)) ) # plot a shaded region for 89% HPDI
abline(v = 0, lty = 2)
```


###Counterfactual Plots

At its simplist level, a **counterfactual plot** is similar to a predictor residual plot in that is allows for statistical control of the other predictor(s). The only difference is that instead of looking at the residuals, we look at the "ceteris paribus" effect of the predictor on the value of the response.

In order to create a counterfactual plot, we set the "controlled for" variables as equal to the mean response, or, if the data is standardized, 0. For the "ceteris paribus" variable(s), we can take as inputs the available data, or unobserved/impossible extrapolations, etc.

```{r}
posterior <- as.data.frame(fit)
mu.link <- function(A, M) posterior$a + posterior$bA*(A) + posterior$bM * M
# prepare new counterfactual data
M_seq <- seq( from=-2 , to=3 , length.out=30 )
pred_data <- data.frame( M = M_seq , A = 0 )
#compute counterfactual expected divorce rate (mu)
mu <- mapply(mu.link, M = pred_data$M, A = pred_data$A) #note use of mapply
  mu_mean <- apply( mu , 2 , median )
  mu_PI <- apply( mu , 2 , PI )
#compute counterfactual predicted divorce rate (y)
sim.response <- mapply(function(A, M)
  rnorm(
    n=nrow(posterior) ,
    mean=posterior$a + posterior$bA*(A) + posterior$bM * M,
    sd=posterior$sigma ), M = pred_data$M, A = pred_data$A   )
D_PI <- apply( sim.response , 2 , PI , prob=0.89 ) 
# display predictions, hiding raw data with type="n"
plot( as.numeric(scale(d$Divorce)) ~ as.numeric(scale(d$Marriage)) , data=d,
      xlab = "Marriage rate (standardized)", ylab = "Divorce rate (standardized)")
mtext( "Counterfactual Effect of Proportion Married on Divorce Rate" )
lines( M_seq , mu_mean )
shade( mu_PI , M_seq )
shade( D_PI , M_seq )
```



###Posterior Prediction Plot

A **posterior prediction plot** measures the relationship between the predicted response and the actual response. It includes all of the "raw data" predictors in the model.

```{r}
mu.link <- function(A, M) posterior$a + posterior$bA*(A) + posterior$bM * M
#using original data
mu <- mapply(mu.link, M = as.numeric(scale(d$Marriage)), A= as.numeric(scale(d$MedianAgeMarriage))) #note use of mapply
mu_mean <- apply( mu , 2 , mean )
mu_PI <- apply( mu , 2 , PI )
# simulate observations
# again no new data, so uses original data
sim.response <- mapply(function(A, M)
  rnorm(
    n=nrow(posterior) ,
    mean=posterior$a + posterior$bA*(A) + posterior$bM * M,
    sd=posterior$sigma ), M = as.numeric(scale(d$Marriage)), A = as.numeric(scale(d$MedianAgeMarriage))   )
#summary stats
D_PI <- apply( sim.response , 2 , PI , prob=0.89 ) #you can also use quartile
#plot
plot( mu_mean ~ as.numeric(scale(d$Divorce)) , col=rangi2 , ylim=range(mu_PI) ,
xlab="Observed divorce" , ylab="Predicted divorce" )
abline( a=0 , b=1 , lty=2 )
for ( i in 1:nrow(d) ) lines( rep(scale(d$Divorce)[i],2) , mu_PI[,i] , col=rangi2 )
```


#Multivariate Regression with a Single Categorical Variable

Data is on different kinds of monkeys ("clade") and the kilocalories of milk. 


##Data

```{r}
setwd("./5. Single Categorical")
data(milk)
d <- milk
unique(d$clade)
clade_id <- as.integer(d$clade) #convert categorical variable to integer
N <- length(clade_id)
N_clade_id <- length(unique(clade_id))
K <- scale(d$kcal.per.g)
K <- as.numeric(K)
stan_rdump(c("clade_id", "N_clade_id", "N", "K"), file="linear_reg.data.R")
data <- read_rdump("linear_reg.data.R")
```

##Stan Model

Note that all of the categories have the same prior. The regression is just mu = the categorical variable.

```{r}
setwd("./5. Single Categorical")
writeLines(readLines("categorical.stan"))
```

```{r}
setwd("./5. Single Categorical")
fit <- stan(file='categorical.stan', data=data, seed=194838, 
            chains = 1, warmup=1000, iter=10000)
```

##Analysis

```{r}
posterior <- as.data.frame(fit)
print(fit, c("sigma", "a"))
plot(fit, pars = c("a", "sigma"), ci_level = .8, outer_level = 0.95, show_density = TRUE) + 
  ggtitle("Posterior Distribution of Coefficients")
```

#Instrumental Variables

From chapter 8.

##Data
```{r}
setwd("./7. Instrumental Variables")
set.seed(73)
N <- 200
U <- rnorm( N ) #U not actually included as data
Q <- rbern( N , prob=0.25 ) #out of rethinking package...
E <- rnorm( N , U - Q ) #E is a fn of U
W <- rnorm( N , U + 0*E ) #W is a fn of U
id <- 1:N #dummy index for person
stan_rdump(c("N", "Q", "E", "W", "id"), file="iv_reg.data.R")
data <- read_rdump("iv_reg.data.R")
```

##Model
```{r}
setwd("./7. Instrumental Variables")
writeLines(readLines("iv_code.stan"))
```

Fit model

```{r}
setwd("./7. Instrumental Variables")
fit <- stan(file='iv_code.stan', data=data, chains = 1, warmup=1000, iter=10000)
```

##Analysis

```{r}
print(fit, c("aW", "bEW", "sigmaW", "aE", "bQE", "sigmaE"))
```

#Multi-level Model



##Data

```{r}
setwd("./13. Multi-Level Models")
data(reedfrogs)
d <- reedfrogs
surv <- d$surv
density <- d$density
tank <- 1:nrow(d)
N = nrow(d)
N_tank = nrow(d)
stan_rdump(c("N", "N_tank", "surv", "density", "tank"), file="linear_reg.data.R")
data <- read_rdump("linear_reg.data.R")
```

##Stan Model

```{r}
setwd("./13. Multi-Level Models")
writeLines(readLines("m12_1.stan"))
```

It is worth noting the difference between a standard regression and multi-level regression at the level of the code. 

In the standard regression, all "varying intercept" coefficients for the categorical variable have the same normal prior. Thus, no information is shared among the coefficients.  

In the multi-level model, the prior has a hyperprior (an "adaptive regularizing prior"), a_tank ~ N(alpha,sigma), with two hyperparameters, a and sigma. The hyperprior models the population of all groups- it makes explicit that all of the intercepts come from the same population. In order to determine the value of the hyperparameters, information from all of the varying intercepts must be pooled. This is done simultaneously with determining the varying intercepts, brought to you by MCMC.

```{r}
setwd("./13. Multi-Level Models")
writeLines(readLines("stan_script.stan"))
```

```{r}
setwd("./13. Multi-Level Models")
fit <- stan(file='stan_script.stan', data=data, chains = 1, warmup=1000, iter=10000)
```

##Analysis

```{r}
print(fit, c("a", "sigma"))
```

